# secureeval: Evaluation and Benchmarking Framework for R LLM Agents

Evaluation and benchmarking framework for R LLM agents. Test agents
against known scenarios, score guardrail accuracy with precision,
recall, and F1 metrics, run regression tests, and compare runs across
iterations.

## See also

Useful links:

- <https://github.com/ian-flores/secureeval>

- Report bugs at <https://github.com/ian-flores/secureeval/issues>

## Author

**Maintainer**: Ian Flores Siaca <iflores.siaca@hey.com>
