# securebench: Guardrail Benchmarking for R LLM Agents

Benchmarking framework for guardrail accuracy in R LLM agent workflows.
Evaluate guardrails against labeled datasets, compute precision, recall,
and F1 metrics, generate confusion matrices, compare results across
iterations, and export as 'vitals'-compatible scorers.

## See also

Useful links:

- <https://ian-flores.github.io/securebench/>

- <https://github.com/ian-flores/securebench>

- Report bugs at <https://github.com/ian-flores/securebench/issues>

## Author

**Maintainer**: Ian Flores Siaca <iflores.siaca@hey.com>
