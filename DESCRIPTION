Package: securebench
Title: Guardrail Benchmarking for Large Language Model Agents
Version: 0.1.0
Authors@R:
    person("Ian", "Flores Siaca", , "iflores.siaca@hey.com", role = c("aut", "cre", "cph"))
Description: Benchmarking framework for guardrail accuracy in R Large
    Language Model (LLM) agent workflows. Evaluate guardrails against labeled
    datasets, compute precision, recall, and F1 metrics, generate confusion
    matrices, compare results across iterations, and export as
    'vitals'-compatible scorers.
License: MIT + file LICENSE
URL: https://ian-flores.github.io/securebench/,
    https://github.com/ian-flores/securebench
BugReports: https://github.com/ian-flores/securebench/issues
Depends:
    R (>= 4.1.0)
Imports:
    cli (>= 3.0.0),
    rlang (>= 1.0.0),
    S7 (>= 0.2.0)
Suggests:
    knitr,
    rmarkdown,
    secureguard,
    testthat (>= 3.0.0),
    vitals,
    withr
Config/testthat/edition: 3
Encoding: UTF-8
Roxygen: list(markdown = TRUE)
Language: en-US
Config/Needs/website: pkgdown
RoxygenNote: 7.3.3
VignetteBuilder: knitr
